# -*- coding: utf-8 -*-
"""big_mart_prediction_assignm1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_S_Rlvr91n9HX04heLTxozpemX5je2rz
"""

# Commented out IPython magic to ensure Python compatibility.



# importing required libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline
import warnings
warnings.filterwarnings('ignore')

train=pd.read_csv('train_XnW6LSF.csv')  #this line reads the data in the train csv file
test=pd.read_csv('test_FewQE9B.csv')  #also this one reads the data in the test csv file

data = pd.concat([train, test])               #the pd.concat combines the two dataframe into a single dataframe called data
print(train.shape, test.shape, data.shape)   #prints the shape of this 3 dataframes, this is to verify that comcatenation has been performed,
                                              # by displaying the shape of this three

#this shows the first five rows of train with our target variable"sales"

train.head()

#this shows the first five rows of test

test.head()

data.head()  #shows the first 5 rows

#we are checking the missing values in data
# we have 2439 missing values in item_weight and so on
#we use this function

data.isnull().sum()

#we are checking the data types

data.dtypes

"""2.filling in missing values"""

#This code is designed to find and list the names of columns in a dataset that contain categorical data.
#Categorical data are types of data that represent categories or labels

cat_col = []
for x in data.dtypes.index:
    if data.dtypes[x] == 'float':
        cat_col.append(x)
cat_col

#we are checking if in the item_weight is there any miss/null/0 data

miss_bool = data['Item_Weight'].isnull()
miss_bool

#we are finding the missing values with mean


data['Item_Weight'].fillna(data['Item_Weight'].mean(), inplace=True)
data['Item_Outlet_Sales'].fillna(data['Item_Outlet_Sales'].mean(), inplace=True)

#we check the missing values and yu can see there are no missing values

data.isnull().sum()

#filling the missing values of categorial variable with mode

data['Outlet_Size'].fillna(data['Outlet_Size'].mode()[0], inplace=True)

#we checking if the is still any missing values after imputation,
#and since we had  4016 missing outlet_sizes we used the fillana fuction to fill the missing values
#now we want to see if we still have missing outlet size
#using this function

data.isnull().sum()

sum(data['Item_Visibility']==0)

# replace zeros with mean
data.loc[:, 'Item_Visibility'].replace([0], [data['Item_Visibility'].mean()], inplace=True)

sum(data['Item_Visibility']==0)

# combine item fat content
data['Item_Fat_Content'] = data['Item_Fat_Content'].replace({'LF':'Low Fat', 'reg':'Regular', 'low fat':'Low Fat'})
data['Item_Fat_Content'].value_counts()

"""CREATION OF NEW ATTRIBUTES"""

data['New_Item_Type'] = data['Item_Identifier'].apply(lambda x: x[:2])
data['New_Item_Type']

data['New_Item_Type'] = data['New_Item_Type'].map({'FD':'Food', 'NC':'Non-Consumable', 'DR':'Drinks'})
data['New_Item_Type'].value_counts()

data.loc[data['New_Item_Type']=='Non-Consumable', 'Item_Fat_Content'] = 'Non-Edible'
data['Item_Fat_Content'].value_counts()

# small values for establishment year
data['Outlet_Years'] = 2013 - data['Outlet_Establishment_Year']

data['Outlet_Years']

data.head()

"""LABEL ENCODING

"""

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
data['Outlet'] = le.fit_transform(data['Outlet_Identifier'])
cat_col = ['Item_Fat_Content', 'Item_Type', 'Outlet_Size', 'Outlet_Location_Type', 'Outlet_Type', 'New_Item_Type']
for col in cat_col:
    data[col] = le.fit_transform(data[col])

"""ONEHOT ENCODING"""

data = pd.get_dummies(data, columns=['Item_Fat_Content', 'Outlet_Size', 'Outlet_Location_Type', 'Outlet_Type', 'New_Item_Type'])
data.head()

"""INPUT SPLIT

"""



X = data.drop(columns=['Outlet_Establishment_Year', 'Item_Identifier', 'Outlet_Identifier', 'Item_Outlet_Sales'])
y = data['Item_Outlet_Sales']

"""CREATING TRAINING AND VALIDATION SET"""

import sklearn
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Creating training and validation set

# stratify will make sure that the distribution of classes in train and validation set it similar
# random state to regenerate the same train and validation set
# test size 0.2 will keep 20% data in validation and remaining 80% in train set

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# shape of training and validation set
(X_train.shape, y_train.shape), (X_test.shape, y_test.shape)

"""2.DEFINING ARCHITECTURE OF THE MODEL"""

# checking the version of keras
import keras
print(keras.__version__)

# checking the version of tensorflow
import tensorflow as tf
print(tf.__version__)

# importing the sequential model
from keras.models import Sequential

# importing different layers from keras
from keras.layers import InputLayer, Dense

# number of input neurons
X_train.shape

X_train.shape[1]

input_neurons = X_train.shape[1]

# define number of output neurons
output_neurons = 1

# Define model
model = Sequential()
model.add(Dense(500, input_dim=22, activation= "relu"))
model.add(Dense(100, activation= "relu"))
model.add(Dense(50, activation= "relu"))
model.add(Dense(1))
model.summary() #Print model Summary

# define hidden layers and neuron in each layer
number_of_hidden_layers = 2
neuron_hidden_layer_1 = 10
neuron_hidden_layer_2 = 5

# defining the architecture of the model
model = Sequential()
model.add(InputLayer(input_shape=(input_neurons,)))
model.add(Dense(units=neuron_hidden_layer_1, activation='relu'))
model.add(Dense(units=neuron_hidden_layer_2, activation='relu'))
model.add(Dense(units=output_neurons, activation='relu'))

# summary of the model
model.summary()

# number of parameters between input and first hidden layer

input_neurons*neuron_hidden_layer_1

# number of parameters between input and first hidden layer

# adding the bias for each neuron of first hidden layer

input_neurons*neuron_hidden_layer_1 + 10

# number of parameters between first and second hidden layer

neuron_hidden_layer_1*neuron_hidden_layer_2 + 5

# number of parameters between second hidden and output layer

neuron_hidden_layer_2*output_neurons + 1

"""compiling the model(defining loss)"""

model.compile(loss= "mean_squared_error" , optimizer="adam", metrics=["mean_squared_error"])
model_history =model.fit(X_train, y_train, epochs=20)

pred_train= model.predict(X_train)
print(np.sqrt(mean_squared_error(y_train,pred_train)))

pred= model.predict(X_test)
print(np.sqrt(mean_squared_error(y_test,pred)))

"""VISUALISING MODEL PERFORMANCE"""

# summarize history for loss
plt.plot(model_history.history['loss'])
plt.plot(model_history.history['loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()